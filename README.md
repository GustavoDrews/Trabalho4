# üé¨ Trabalho 4 ‚Äì Processamento Paralelo de Dataset de Filmes (MPI + OpenMP)

Desenvolvido para a disciplina de **Programa√ß√£o Paralela**, este projeto realiza o processamento distribu√≠do de um dataset de filmes utilizando:

- **MPI (Message Passing Interface)** ‚Äî paralelismo de mem√≥ria distribu√≠da
- **OpenMP** ‚Äî paralelismo de mem√≥ria compartilhada
- **Vers√£o h√≠brida (MPI + OpenMP)**
- Estat√≠sticas sobre avalia√ß√£o dos filmes em portugu√™s e **Top 30 filmes mais bem avaliados**
- An√°lise de desempenho com **tempo**, **speedup** e **efici√™ncia**

---

# üì¶ O que este projeto faz

1. **L√™ e distribui o CSV de filmes** entre os processos MPI.
2. Cada processo:
   - filtra e processa apenas seu subconjunto de linhas;
   - calcula estat√≠sticas locais (filmes em portugu√™s com contagem e m√©dia de popularidade);
   - mant√©m um Top-30 local.
3. Os m√©todos **`MPI_Reduce`** e  **`MPI_Gather`** re√∫nem resultados parciais.
4. O processo **rank 0** monta o **Top 30 final** entre todos os processos.
5. Vers√µes:
   - `movies_mpi.c` ‚Üí vers√£o s√≥ MPI
   - `movies_mpi_opm.c` ‚Üí vers√£o h√≠brida com OpenMP

---

# ‚úÖ Pr√©-requisitos
Instale os pacotes base (C, MPI,OpenMP)
```bash
sudo apt update
sudo apt install -y build-essential openmpi-bin libopenmpi-dev
```
**Observa√ß√£o:** O projeto foi pensando para WSL.

# ‚öôÔ∏è Compila√ß√£o

Vers√£o simplificada
```bash
make
```
Gera os arquivos:
- `build/movies_mpi`
- `build/movies_mpi_opm`

Compila√ß√£o Manual(opcional)
```bash
mpicc -O2 -Wall -o build/movies_mpi src/movies_mpi.c
mpicc -O2 -Wall -fopenmp -o build/movies_mpi_opm src/movies_mpi_opm.c

```

# ‚ñ∂Ô∏è Execu√ß√£o
Dataset usado:

```bash
data/full_top_rated_movies.csv
```
1) Executar somente MPI
Sequencial - 1 processo
```bash
export OMP_NUM_THREADS=1
mpirun -np 1 ./build/movies_mpi data/full_top_rated_movies.csv
```
Paralelo - 4 processos
```bash
export OMP_NUM_THREADS=1
mpirun -np 4 ./build/movies_mpi data/full_top_rated_movies.csv
```
2) Executar vers√£o h√≠brida MPI + OpenMP
Exemplo com 4 processos x 4threads
```bash
export OMP_NUM_THREADS=4
mpirun -np 4 ./build/movies_mpi_opm data/full_top_rated_movies.csv

```
A sa√≠da de todas as vers√µes √© composta por:
- A contagem dos filmes em portugu√™s
- M√©dia de popularidade destes
- Top30 filmes mais bem avaliados
- Tempo total de execu√ß√£o

# üìä An√°lise das M√©tricas de Desempenho
### üßÆ F√≥rmulas Utilizadas:

**Speedup (\(S_p\))**

$$
S_p = \frac{T_1}{T_p}
$$

**Efici√™ncia (\(E_p\))**

$$
E_p = \frac{S_p}{P}
$$



Foram medidos os seguintes tempos:
A implementa√ß√£o com apenas MPI mostra um comportamento mais esperado:

- De 1 para 2 processos h√° uma melhoria real (speedup ‚âà 1.48).

- Com 4 processos ainda h√° ganho, mas menor.

- A partir de 8 processos o desempenho come√ßa a degradar.

- Em 12 processos o speedup cai abaixo de 1 (overhead maior que o benef√≠cio).


1) Desempenho do `movie_mpi.c`

| # Processos | Tempo (s)  | Speedup | Efici√™ncia |
|-------------|------------|---------|------------|
| 1           | 0.007978   | 1.000   | 1.000      |
| 2           | 0.005404   | 1.476   | 0.738      |
| 4           | 0.005686   | 1.403   | 0.351      |
| 8           | 0.006428   | 1.241   | 0.155      |
| 12          | 0.009006   | 0.886   | 0.074      |

O programa tem uma pequena janela de escalabilidade eficiente, entre 2 e 4 processos.

Para muitos processos, MPI adiciona overhead de comunica√ß√£o e sincroniza√ß√µes que superam o ganho.

Tamb√©m pode haver oversubscribe, especialmente se sua m√°quina n√£o tem 12 n√∫cleos f√≠sicos.

**Resultado:** Escalabilidade moderada para poucos processos, mas negativa para muitos.

---

2) Desempenho do `movie_mpi_opm.c`

A tabela mostra que a vers√£o h√≠brida (MPI + OpenMP) apresenta piora significativa de desempenho conforme o n√∫mero de processos aumenta:

- O tempo aumenta em vez de diminuir.

- O speedup cai para menos de 1 j√° com 2 processos.

- A efici√™ncia despenca rapidamente (de 23% com 2 processos para menos de 1% com 12 processos).

| # Processos | Tempo (s)  | Speedup | Efici√™ncia |
|-------------|------------|---------|------------|
| 1           | 0.016815   | 1.000   | 1.000      |
| 2           | 0.035435   | 0.475   | 0.237      |
| 4           | 0.056498   | 0.298   | 0.074      |
| 8           | 0.117992   | 0.143   | 0.018      |
| 12          | 0.193663   | 0.087   | 0.007      |

Isso indica que o uso combinado de MPI + OpenMP est√° adicionando mais overhead do que benef√≠cio. Poss√≠veis causas:

- O problema √© pequeno demais para justificar paraleliza√ß√£o h√≠brida.

- H√° custo alto de comunica√ß√£o entre processos MPI.

- As regi√µes paralelas OpenMP talvez sejam pequenas, ou existam opera√ß√µes serializadas.

 - O ambiente est√° oversubscribed, causando competi√ß√£o por CPU.

- A divis√£o do trabalho n√£o compensa o custo de coordena√ß√£o.

**Resultado:** Escalabilidade negativa

# üîÅ Resumo - Comandos essenciais

```bash
# 1) compilar
make

# 2) MPI sequencial
mpirun -np 1 ./build/movies_mpi data/full_top_rated_movies.csv

# 3) MPI paralelo
mpirun -np 4 ./build/movies_mpi data/full_top_rated_movies.csv

# 4) H√≠brido MPI + OpenMP
export OMP_NUM_THREADS=4
mpirun -np 4 ./build/movies_mpi_opm data/full_top_rated_movies.csv
```
